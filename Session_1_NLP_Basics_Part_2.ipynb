{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coezbek/uts-36118-anlp-2026/blob/main/Session_1_NLP_Basics_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tinyurl.com/ANLPcolab1part2\n",
        "\n",
        "\n",
        "Go to \"File\" -> \"Save a Copy in Drive...\"\n",
        "This lets you create your own copy of the notebook in your Google drive, and any changes you make doesn't impact the shared notebook"
      ],
      "metadata": {
        "id": "mR2O-rQ6dWuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic text analysis using Python"
      ],
      "metadata": {
        "id": "C-lUWaphEwK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to install the required libraries using the pip command (if you don't have them), and import the modules from the libraries.\n",
        "\n"
      ],
      "metadata": {
        "id": "siLpslJoEXEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enable plots to be displayed in the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install seaborn\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "FFx6fp41EVoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6572d2-ec21-4e46-b7fb-f5586b6ae4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpNlkY77RMcQ"
      },
      "source": [
        "## Mounting the drive\n",
        "\n",
        "In this notebook, I'm mounting the Google drive to read a csv file that is stored on my drive. You must allow access to your drive by signing in to your Google account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBjL_EPjRKla",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "d7da1752-ac33-440e-aa29-9107d2d45058"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset from here: https://drive.google.com/file/d/1qAC9x-WMwzofyG8l1fFUoHwWhk3n61fp/view?usp=sharing\n",
        "\n",
        "Then, copy it to your Google drive folder which contains the notebook"
      ],
      "metadata": {
        "id": "aUdB-OV3rUkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After executing the cell above, Drive files will be present in \"/content/drive/My Drive\". The below command lists the contents in the drive:\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/ANLP Colab Notebooks/ANLP Datasets\"\n"
      ],
      "metadata": {
        "id": "WQmrB47_ugt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRuOvT-KR5hX"
      },
      "source": [
        "## Reading Data from a CSV File\n",
        "\n",
        "To read the data from the input csv file from my Google drive and store it as a Python dataframe, I use the read_csv() function from Pandas. You have to change the folder location to where the file is stored in your own Gdrive - mine is in this path:\n",
        "/content/drive/My Drive/Colab Notebooks/ANLP Datasets/Session1_CNN_Articles_2021-2023.csv\n",
        "\n",
        "You can read about the different functions and their input parameters in the  documentation for the library:\n",
        "[Pandas Documentation](http://pandas.pydata.org/pandas-docs/stable/)\n",
        "\n",
        "**Note:** Comment code below if you are not importing from your Gdrive folder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The input csv file is a subset of the data from https://github.com/hadasu/CNN_web_crawler\n",
        "#MAKE SURE YOU CHANGE THIS FOLDER TO POINT TO THE RIGHT DIRECTORY IN YOUR GDRIVE\n",
        "newsdf = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ANLP Colab Notebooks/ANLP Datasets/Session1_CNN_Articles_2021-2023.csv',encoding='unicode_escape')\n",
        "newsdf.describe()"
      ],
      "metadata": {
        "id": "mNyLH32duQZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading input file from a url\n",
        "The alternative option is to read in the CSV from a web url (on github) and store it in a dataframe. This is a smaller dataset containing articles only from 2021 January to March.\n"
      ],
      "metadata": {
        "id": "67DM2moqmuSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/AntonetteShibani/NLPAnalysis/blob/main/CNN_Articles_2021.csv?raw=true'\n",
        "newsdf = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "ABIe5ntOo6lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary data inspection\n",
        "\n",
        "We usually try to get a a sense of the data first (particularly useful for large data sets where opening in other UI based tools is not easy)"
      ],
      "metadata": {
        "id": "qG-Q1XGgKh4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print general information about a DataFrame including the index dtype and columns, non-null values and memory usage\n",
        "newsdf.info()"
      ],
      "metadata": {
        "id": "RD7R1zatKX4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsdf.rename(columns={'Unnamed: 0': 'ID'}, inplace=True)"
      ],
      "metadata": {
        "id": "BHnSFho0v9y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate descriptive statistics that summarizes the central tendency, dispersion and shape of a datasetâ€™s distribution\n",
        "newsdf.describe()"
      ],
      "metadata": {
        "id": "AE0pZ_0yLNRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHKzU83SzuWS"
      },
      "source": [
        "# Use the .head(n) function to look at the first 'n' rows of our news dataframe. The default n is 5, we are now changing it to view the first 10 rows\n",
        "newsdf.head(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A function similar to above, but provides a random sample of rows rather than the first few.\n",
        "newsdf.sample(5)"
      ],
      "metadata": {
        "id": "Va1qa7rtLq0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Count\n",
        "\n",
        "Word counts are simple but useful indicators for asking questions on the length of texts.\n",
        "\n",
        "To demonstrate usage, we see how the metrics are calculated for one sample sentence from the dataset."
      ],
      "metadata": {
        "id": "ne2j1DzGJy7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = newsdf['headline'][2]\n",
        "print(s)\n",
        "\n",
        "#Splitting by whitespace characters and calculating the length. Note that punctuation marks are also counted as words\n",
        "len(s.split())"
      ],
      "metadata": {
        "id": "Coae8Z9hpWUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To make it easier to reuse in the future, we can create a function that returns word count\n",
        "def word_count(text):\n",
        "    wc = len(text.split())\n",
        "    return wc"
      ],
      "metadata": {
        "id": "QbJSpFNEOhjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now now we can apply the word_count function to our text variable to create a new variable with the number of words in the news article text."
      ],
      "metadata": {
        "id": "Qhe5E9UDaiTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsdf['article_word_count'] = newsdf['text'].apply(word_count)"
      ],
      "metadata": {
        "id": "4VAJynJ4br3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use describe, hist, and scatter functions to provide some information on the length of articles in our dataset"
      ],
      "metadata": {
        "id": "RTgA9Z4_cxvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsdf['article_word_count'].describe()"
      ],
      "metadata": {
        "id": "SOd1nHTebsAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsdf['article_word_count'].hist(bins = 10)"
      ],
      "metadata": {
        "id": "oXvBS47ic_th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"part_of\",\n",
        "            y = \"article_word_count\",\n",
        "            palette='Set3',\n",
        "            data =newsdf);"
      ],
      "metadata": {
        "id": "juhl86IXdHX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I'm using a function that populates bar graph from a dataframe variable\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def wordBarGraphFunction(df,column,title):\n",
        "    topic_words = [ z.lower() for y in\n",
        "                       [ x.split() for x in df[column] if isinstance(x, str)]\n",
        "                       for z in y]\n",
        "    word_count_dict = dict(Counter(topic_words))\n",
        "    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n",
        "    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n",
        "    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n",
        "    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cmFwrYRqsQ44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "wordBarGraphFunction(newsdf,'headline',\"Most frequent words in news article headlines (Jan-Mar 2021)\")"
      ],
      "metadata": {
        "id": "4tgDzrXHsccM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can further explore the articles which are of the longest and shortest lengths"
      ],
      "metadata": {
        "id": "XBRZc7rHritQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#shortest\n",
        "newsdf.sort_values(by='article_word_count').head(10)"
      ],
      "metadata": {
        "id": "onZCq-mHrn5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#longest\n",
        "newsdf.sort_values(by='article_word_count', ascending=False).head(10)"
      ],
      "metadata": {
        "id": "qtDpdf1NsKoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then examine the content of individual articles to gain additional insight as needed."
      ],
      "metadata": {
        "id": "h2v79HwStMGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word frequencies\n",
        "\n",
        "Word frequencies (counting how often words occur) is a critical step in quantifying texts for many kinds of text analysis. There are inbuilt functions in Python that can compute words frequencies.\n",
        "\n",
        "Note that this analysis disregards the word order in the original sentence, taking a bag-of-words approach.\n"
      ],
      "metadata": {
        "id": "aEammaw6w257"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hymcINsjgsC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate frequencies to determine the most common words in the corpus"
      ],
      "metadata": {
        "id": "Ugg2B30rpXA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting series to string\n",
        "article_text = newsdf['text'].to_string()\n",
        "\n",
        "#create word tokens\n",
        "tokenized_words=word_tokenize(article_text)"
      ],
      "metadata": {
        "id": "4UrTROBE_-u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words=nltk.FreqDist(tokenized_words)\n",
        "all_words.plot(10);\n",
        "print(all_words.most_common(20))"
      ],
      "metadata": {
        "id": "_oCBNfxQ-1Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a word cloud to show most common words\n",
        "\n",
        "Note: There are so many ways in which you can customise word clouds for display, check out the documentation and read related blogs posts to try different combinations. Here, I'm using the wordcloud package to create a word cloud from the given article text."
      ],
      "metadata": {
        "id": "wIlWUjGRSlFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(max_words=100).generate(article_text)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PPMUWOkISiyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8tDlcK6UWHi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will notice that the most frequent terms are stopwords and punctuations, let's try recalculating frequencies after performing some basic cleaning."
      ],
      "metadata": {
        "id": "--Kr_8YULsjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting article text to lowercase as Python is case-sensitive\n",
        "article_text_lower = article_text.lower()\n",
        "\n",
        "#create word tokens\n",
        "tokenized_words=word_tokenize(article_text_lower)\n",
        "\n",
        "#Set up stop words for removal\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#stopwords\n",
        "stop_words=stopwords.words(\"english\")\n",
        "print(stop_words)\n",
        "#Add custom stopwords to the list\n",
        "stop_words.extend([\"cnn\", \"'s\", \"a\", \"the\"])"
      ],
      "metadata": {
        "id": "jYscHfPDNPJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a new variable to store filtered tokens\n",
        "filtered_tokens=[]\n",
        "for w in tokenized_words:\n",
        "    if w not in stop_words:\n",
        "         #add all filtered tokens excluding stopwords in this list below\n",
        "         filtered_tokens.append(w)\n",
        "\n",
        "import string\n",
        "# punctuations\n",
        "punctuations=list(string.punctuation)\n",
        "\n",
        "#Add custom punctuations to the list by running pre-processing steps with the data set and adding relevant ones\n",
        "punctuations.append(\"...\")\n",
        "punctuations.append(\"``\")\n",
        "punctuations.append(\"''\")\n",
        "\n",
        "print(\"List of punctuations to remove:\\n\")\n",
        "print(punctuations)\n",
        "\n",
        "#Create another variable to store all clean tokens\n",
        "filtered_tokens_clean=[]\n",
        "for i in filtered_tokens:\n",
        "    if i not in punctuations:\n",
        "        filtered_tokens_clean.append(i)"
      ],
      "metadata": {
        "id": "xDdJoMlmL5Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have cleaned the input text, let's calculate frequencies again to view the most common words."
      ],
      "metadata": {
        "id": "3I6rctiFQ0ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words=nltk.FreqDist(filtered_tokens_clean)\n",
        "all_words.plot(10);\n",
        "print(all_words.most_common(20))"
      ],
      "metadata": {
        "id": "2iimW5OLOM5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate the word cloud again with the cleaned set of words. Here, I'm creating the word cloud from the word frequencies we calculated from the last step (rather than passing the entire text)."
      ],
      "metadata": {
        "id": "M4Otpvzjv4Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert list of tuples to dictionary\n",
        "word_freq = dict(all_words)\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='black').generate_from_frequencies(word_freq)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9uGf5RnnvUqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: What are the insights from here? What do the key words indicate?"
      ],
      "metadata": {
        "id": "XleKubsVRCGt"
      }
    }
  ]
}